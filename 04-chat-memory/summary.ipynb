{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7290e3a",
   "metadata": {},
   "source": [
    "## LangChain Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c00ed0",
   "metadata": {},
   "source": [
    "- ConversationBufferMemory \n",
    "    - It's like a list with all the messages\n",
    "- ConversationBufferWindowMemory\n",
    "    - Quite similar to the above one, but returns just only a chunk of the list\n",
    "- ConversationSummaryMemory\n",
    "    - Instead of storing the complete messages, it stores only a summary of the conversation\n",
    "- ConversationSummaryBufferMemory\n",
    "    - Combination between ConversationSummaryMemory and ConversationBufferWindowMemory\n",
    "    You get a summary of the conversation + Xk of recent tokens stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8040aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcfb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eec74f",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory - DEPRECATED WAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2d8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ea051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis\\AppData\\Local\\Temp\\ipykernel_16472\\1952740995.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb622e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis\\AppData\\Local\\Temp\\ipykernel_16472\\3822530700.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3adfee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello there my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James. The AI greets James and expresses its willingness to chat and help, inquiring about how his day is going.\n",
      "Human: I am researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James, and the AI greets him, expressing its willingness to chat and asking about his day. James mentions he is researching different types of conversational memory. The AI explains that conversational memory includes short-term memory (retaining context within a single conversation), long-term memory (retaining information across multiple interactions), contextual memory (remembering the context and emotional tone of conversations), and dynamic memory (adapting based on new information). The AI then asks if James is looking into a specific type of conversational memory or exploring the topic more broadly.\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James, and the AI greets him, expressing its willingness to chat and asking about his day. James mentions he is researching different types of conversational memory. The AI explains that conversational memory includes short-term memory (retaining context within a single conversation), long-term memory (retaining information across multiple interactions), contextual memory (remembering the context and emotional tone of conversations), and dynamic memory (adapting based on new information). The AI then asks if James is looking into a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI describes ConversationBufferMemory as tracking recent interactions to maintain context in short conversations, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for flexible context management. The AI then inquires whether James is focusing on how these memory types enhance user experience or their technical implementations.\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James, and the AI greets him, expressing its willingness to chat and asking about his day. James mentions he is researching different types of conversational memory. The AI explains that conversational memory includes short-term memory (retaining context within a single conversation), long-term memory (retaining information across multiple interactions), contextual memory (remembering the context and emotional tone of conversations), and dynamic memory (adapting based on new information). The AI then asks if James is looking into a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI describes ConversationBufferMemory as tracking recent interactions to maintain context in short conversations, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for flexible context management. The AI then inquires whether James is focusing on how these memory types enhance user experience or their technical implementations. James notes that buffer memory just stores the entire conversation, to which the AI agrees, explaining that while ConversationBufferMemory retains the entire conversation history for context, it can lead to increased memory usage in lengthy discussions. The AI highlights the advantages of ConversationBufferWindowMemory in limiting stored messages to maintain relevant context while ensuring a coherent experience. It then asks if James is considering the trade-offs between these memory types in his research.\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
       " 'history': 'The human introduces himself as James, and the AI greets him, expressing its willingness to chat and asking about his day. James mentions he is researching different types of conversational memory. The AI explains that conversational memory includes short-term memory (retaining context within a single conversation), long-term memory (retaining information across multiple interactions), contextual memory (remembering the context and emotional tone of conversations), and dynamic memory (adapting based on new information). The AI then asks if James is looking into a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI describes ConversationBufferMemory as tracking recent interactions to maintain context in short conversations, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for flexible context management. The AI then inquires whether James is focusing on how these memory types enhance user experience or their technical implementations. James notes that buffer memory just stores the entire conversation, to which the AI agrees, explaining that while ConversationBufferMemory retains the entire conversation history for context, it can lead to increased memory usage in lengthy discussions. The AI highlights the advantages of ConversationBufferWindowMemory in limiting stored messages to maintain relevant context while ensuring a coherent experience. It then asks if James is considering the trade-offs between these memory types in his research.',\n",
       " 'response': \"That's correct! ConversationBufferWindowMemory retains only the most recent k messages, which allows it to maintain a relevant context while discarding older messages that may no longer be pertinent to the current conversation. This approach helps manage memory usage effectively, especially in longer interactions, while still providing a coherent conversational experience. It strikes a balance between context retention and resource efficiency. Are you exploring how these trade-offs impact user engagement or satisfaction in your research?\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello there my name is James\"})\n",
    "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
    "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
    "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
    "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "272e6f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James, and the AI greets him, expressing its willingness to chat and asking about his day. James mentions he is researching different types of conversational memory. The AI explains that conversational memory includes short-term memory (retaining context within a single conversation), long-term memory (retaining information across multiple interactions), contextual memory (remembering the context and emotional tone of conversations), and dynamic memory (adapting based on new information). The AI then asks if James is looking into a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI describes ConversationBufferMemory as tracking recent interactions to maintain context in short conversations, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for flexible context management. The AI then inquires whether James is focusing on how these memory types enhance user experience or their technical implementations. James notes that buffer memory just stores the entire conversation, to which the AI agrees, explaining that while ConversationBufferMemory retains the entire conversation history for context, it can lead to increased memory usage in lengthy discussions. The AI highlights the advantages of ConversationBufferWindowMemory in limiting stored messages to maintain relevant context while ensuring a coherent experience. It then asks if James is considering the trade-offs between these memory types in his research. James points out that buffer window memory stores the last k messages, dropping the rest. The AI confirms this, explaining that ConversationBufferWindowMemory retains only the most recent k messages to maintain relevant context while discarding older messages, thus managing memory usage effectively and providing a coherent conversational experience. The AI then asks if James is exploring how these trade-offs impact user engagement or satisfaction in his research.\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': 'The human introduces himself as James, and the AI greets him, expressing its willingness to chat and asking about his day. James mentions he is researching different types of conversational memory. The AI explains that conversational memory includes short-term memory (retaining context within a single conversation), long-term memory (retaining information across multiple interactions), contextual memory (remembering the context and emotional tone of conversations), and dynamic memory (adapting based on new information). The AI then asks if James is looking into a specific type of conversational memory or exploring the topic more broadly. James shares that he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI describes ConversationBufferMemory as tracking recent interactions to maintain context in short conversations, while ConversationBufferWindowMemory uses a sliding window to dynamically adjust the number of retained messages for flexible context management. The AI then inquires whether James is focusing on how these memory types enhance user experience or their technical implementations. James notes that buffer memory just stores the entire conversation, to which the AI agrees, explaining that while ConversationBufferMemory retains the entire conversation history for context, it can lead to increased memory usage in lengthy discussions. The AI highlights the advantages of ConversationBufferWindowMemory in limiting stored messages to maintain relevant context while ensuring a coherent experience. It then asks if James is considering the trade-offs between these memory types in his research. James points out that buffer window memory stores the last k messages, dropping the rest. The AI confirms this, explaining that ConversationBufferWindowMemory retains only the most recent k messages to maintain relevant context while discarding older messages, thus managing memory usage effectively and providing a coherent conversational experience. The AI then asks if James is exploring how these trade-offs impact user engagement or satisfaction in his research.',\n",
       " 'response': \"Your name is James! How's your research going?\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a194e3",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory - Newest way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad59da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4106463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant called Zeta.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f561afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b88c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de0cb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a8752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        super().__init__(llm=llm)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"\n",
    "        Create a summary\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much as possible relevant information as possible\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template([\n",
    "                \"Existing conversation summary: \\n{existing_summary}\\n\\n\"\n",
    "                \"New messages: \\n{messages}\"\n",
    "            ])\n",
    "        ])\n",
    "\n",
    "        #format the messages and invoke the llm\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages,\n",
    "                messages=[x.content for x in messages] \n",
    "            )\n",
    "        )\n",
    "\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        Clear the history\n",
    "        \"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137a1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exists, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "443a2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc89ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2a6606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Luis! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C26auRE5o1HFTfKYFw9sDBdeuOwr8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--bf146b6f-92d9-4eca-9054-17788dc93582-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hy, my name is Luis\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b39de80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='In the conversation, Luis introduces himself by saying, \"Hy, my name is Luis.\" The AI responds warmly, asking, \"Hi Luis! How can I assist you today?\"', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d291a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Luis! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 65, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C26aw2smBaYwwIbWNWDgLy0Wu4nhq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a5079f89-1e5b-4d0b-9178-a9f23d5886b8-0', usage_metadata={'input_tokens': 65, 'output_tokens': 12, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What's my name again?\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfd65b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='In the conversation, Luis introduces himself by saying, \"Hy, my name is Luis.\" The AI responds warmly, asking, \"Hi Luis! How can I assist you today?\" Luis then asks, \"What\\'s my name again?\" to which the AI replies, \"Your name is Luis! How can I assist you today?\"', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ed603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
