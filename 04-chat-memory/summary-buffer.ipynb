{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566c04b4",
   "metadata": {},
   "source": [
    "## LangChain Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbf955",
   "metadata": {},
   "source": [
    "- ConversationBufferMemory \n",
    "    - It's like a list with all the messages\n",
    "- ConversationBufferWindowMemory\n",
    "    - Quite similar to the above one, but returns just only a chunk of the list\n",
    "- ConversationSummaryMemory\n",
    "    - Instead of storing the complete messages, it stores only a summary of the conversation\n",
    "- ConversationSummaryBufferMemory\n",
    "    - Combination between ConversationSummaryMemory and ConversationBufferWindowMemory\n",
    "    You get a summary of the conversation + Xk of recent tokens stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491b862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fd067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af30a2",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory - DEPRECATED WAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc07a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a484c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BEU7CA\\AppData\\Local\\Temp\\ipykernel_2780\\2411911775.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=300,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "355fd630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BEU7CA\\AppData\\Local\\Temp\\ipykernel_2780\\3822530700.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ed782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Hi, my name is Luis!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi, my name is Luis!',\n",
       " 'history': [HumanMessage(content='Hi, my name is Luis!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello, Luis! It's great to meet you! How's your day going so far?\", additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"Hello, Luis! It's great to meet you! How's your day going so far?\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"Hi, my name is Luis!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb42f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is Luis!', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, Luis! It's great to meet you! How's your day going so far?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: I'm researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content=\"The human, named Luis, introduces himself and shares that he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from previous interactions for a more personalized experience. It outlines five types of conversational memory: short-term memory, which retains information during a single session; long-term memory, which stores information across multiple sessions; contextual memory, which focuses on the conversation's context; dynamic memory, which updates based on new information; and episodic memory, which recalls specific events or interactions. The AI then asks Luis if he is looking into a specific type of conversational memory or exploring them all.\", additional_kwargs={}, response_metadata={})]\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content=\"The human, named Luis, introduces himself and shares that he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from previous interactions for a more personalized experience. It outlines five types of conversational memory: short-term memory, which retains information during a single session; long-term memory, which stores information across multiple sessions; contextual memory, which focuses on the conversation's context; dynamic memory, which updates based on new information; and episodic memory, which recalls specific events or interactions. The AI then asks Luis if he is looking into a specific type of conversational memory or exploring them all.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's great! Both ConversationBufferMemory and ConversationBufferWindowMemory are interesting approaches to managing conversational memory. \\n\\n**ConversationBufferMemory** typically retains all the messages from a conversation in a buffer, allowing the AI to reference previous exchanges during the current session. This can be particularly useful for maintaining context and continuity, especially in longer conversations where details from earlier messages might be relevant later on.\\n\\nOn the other hand, **ConversationBufferWindowMemory** is a bit more selective. It keeps a fixed-size window of the most recent messages, which means that as new messages come in, older ones are discarded. This approach can help manage memory usage and keep the conversation focused on the most relevant context, especially in scenarios where the conversation might become lengthy or when the AI needs to prioritize recent interactions.\\n\\nAre you finding one of these types more useful for your research, or are you exploring their differences in depth?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content='The human, named Luis, introduces himself and shares that he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from previous interactions for a more personalized experience. It outlines five types of conversational memory: short-term memory, long-term memory, contextual memory, dynamic memory, and episodic memory. The AI then asks Luis if he is looking into a specific type of conversational memory or exploring them all. Luis mentions he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI explains that ConversationBufferMemory retains all messages from a conversation for context, while ConversationBufferWindowMemory keeps a fixed-size window of the most recent messages, discarding older ones to manage memory usage. The AI inquires if Luis finds one of these types more useful for his research or if he is exploring their differences in depth.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's correct! Buffer memory, like ConversationBufferMemory, stores the entire conversation history, allowing the AI to access all previous messages during the session. This can be beneficial for maintaining context and ensuring that the AI can refer back to earlier parts of the conversation when needed. However, it can also lead to increased memory usage, especially in longer conversations. \\n\\nIn contrast, ConversationBufferWindowMemory, as I mentioned earlier, only keeps a fixed number of the most recent messages, which helps manage memory more efficiently. \\n\\nAre you considering the trade-offs between these two types in your research?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for message in [\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    chain.invoke({\"input\": message})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60558c3d",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory - Newest way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4481b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38349c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant called Zeta.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc628602",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60f0b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff723b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00c01054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"\n",
    "    Add messages to the history, removing any message\n",
    "    beyond the last 'k' messages and summarizing the messages that we drop\n",
    "    \"\"\"\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None \n",
    "        old_messages = None\n",
    "\n",
    "        # see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        # check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> found {len(self.messages)} messages, dropping\"\n",
    "                f\"oldest {len(self.messages) - self.k} messages\"\n",
    "            )\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[:self.k]\n",
    "            # ...and keep the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "        \n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "        \n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible. However, we need to \"\n",
    "                \"keep our summary concise, the limit is a single short paragraph.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        Clear the history\n",
    "        \"\"\"\n",
    "        self.messages = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62737609",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatOpenAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98bb4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80ab36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=llm\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c02efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> No old messages to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Luis! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 25, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C2HbYXlXnZc9WsvJQ521Uf2h0BOar', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4aabc662-aec1-43b1-8b3e-f448bf983616-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi my name is Luis\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c1b62e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      ">> found 6 messages, droppingoldest 2 messages\n",
      ">> New summary: Luis introduced himself and expressed interest in researching different types of conversational memory. The AI responded by outlining various types of conversational memory, including short-term, long-term, contextual, episodic, semantic, declarative vs. procedural, social, and personal memory, explaining their relevance in both human communication and artificial intelligence. Luis was invited to ask further questions or explore specific aspects of conversational memory.\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> found 6 messages, droppingoldest 2 messages\n",
      ">> New summary: Luis is researching different types of conversational memory and has expressed interest in specific concepts like ConversationBufferMemory and ConversationBufferWindowMemory. The AI provided an overview of various types of conversational memory, including short-term, long-term, contextual, episodic, semantic, declarative vs. procedural, social, and personal memory. It then explained the characteristics and use cases of ConversationBufferMemory, which retains the entire conversation history, and ConversationBufferWindowMemory, which keeps only a limited number of recent messages for efficiency. Luis was encouraged to ask further questions about these memory types or their implementation in AI frameworks.\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> found 6 messages, droppingoldest 2 messages\n",
      ">> New summary: Luis is exploring ConversationBufferMemory and ConversationBufferWindowMemory in conversational AI. The AI provided detailed explanations of both memory types, highlighting that ConversationBufferMemory retains the entire conversation history for context and coherence, while ConversationBufferWindowMemory keeps only a limited number of recent messages to enhance efficiency. The AI also discussed the advantages and limitations of buffer memory, emphasizing its importance in applications requiring detailed context, such as customer support and therapy chatbots, while noting potential issues with memory usage and processing speed. Luis is encouraged to ask further questions about these concepts or their implementation.\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate([\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9efd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
