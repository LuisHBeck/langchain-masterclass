{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901e31fe",
   "metadata": {},
   "source": [
    "## LangChain Streaming - Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3a0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8988746",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm: ChatOpenAI = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.0,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8cabc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'service_tier': 'default'}, id='run--31a9345b-71ba-4cb6-8eff-a8abcdc86c0b-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hello there\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c720753",
   "metadata": {},
   "source": [
    "### Streaming with astream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70ee7f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|N|LP| stands| for| Natural| Language| Processing|,| which| is| a| sub|field| of| artificial| intelligence| (|AI|)| and| computer| science| focused| on| the| interaction| between| computers| and| humans| through| natural| language|.| The| goal| of| NLP| is| to| enable| computers| to| understand|,| interpret|,| generate|,| and| respond| to| human| language| in| a| way| that| is| both| meaningful| and| useful|.\n",
      "\n",
      "|Key| components| of| NLP| include|:\n",
      "\n",
      "|1|.| **|Text| Processing|**|:| In|vol|ves| cleaning| and| preparing| text| data| for| analysis|,| including| token|ization|,| stemming|,| le|mmat|ization|,| and| removing| stop| words|.\n",
      "\n",
      "|2|.| **|Syntax| and| Parsing|**|:| An|aly|zing| the| grammatical| structure| of| sentences| to| understand| relationships| between| words|,| including| part|-of|-s|peech| tagging| and| dependency| parsing|.\n",
      "\n",
      "|3|.| **|Sem|antics|**|:| Understanding| the| meaning| of| words| and| sentences|,| including| word| sense| dis|ambigu|ation| and| semantic| role| labeling|.\n",
      "\n",
      "|4|.| **|Sent|iment| Analysis|**|:| Determ|ining| the| emotional| tone| behind| a| body| of| text|,| often| used| in| social| media| monitoring| and| customer| feedback| analysis|.\n",
      "\n",
      "|5|.| **|Machine| Translation|**|:| Automatically| translating| text| from| one| language| to| another|,| such| as| Google| Translate|.\n",
      "\n",
      "|6|.| **|Text| Generation|**|:| Creating| coherent| and| context|ually| relevant| text| based| on| input| data|,| as| seen| in| chat|bots| and| content| generation| tools|.\n",
      "\n",
      "|7|.| **|Speech| Recognition|**|:| Con|verting| spoken| language| into| text|,| enabling| voice|-|activated| systems| and| virtual| assistants|.\n",
      "\n",
      "|8|.| **|Information| Retrieval|**|:| Extract|ing| relevant| information| from| large| datasets|,| such| as| search| engines| and| recommendation| systems|.\n",
      "\n",
      "|N|LP| combines| techniques| from| lingu|istics|,| computer| science|,| and| machine| learning| to| process| and| analyze| large| amounts| of| natural| language| data|,| making| it| a| crucial| technology| in| various| applications|,| including| chat|bots|,| virtual| assistants|,| sentiment| analysis| tools|,| and| more|.||"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "async for token in llm.astream(\"What is NLP?\"):\n",
    "    tokens.append(token)\n",
    "    print(token.content, end=\"|\", flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903e147e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='N', additional_kwargs={}, response_metadata={}, id='run--1f2142b0-5ef6-48db-8304-06efe40bdae1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac948319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='LP', additional_kwargs={}, response_metadata={}, id='run--1f2142b0-5ef6-48db-8304-06efe40bdae1')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "234675bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='NLP stands for Natural Language Processing,', additional_kwargs={}, response_metadata={}, id='run--1f2142b0-5ef6-48db-8304-06efe40bdae1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1] + tokens[2] + tokens[3] + tokens[4] + tokens[5] + tokens[6] + tokens[7] + tokens[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f2c9ac",
   "metadata": {},
   "source": [
    "### Streaming with Agents\n",
    "\n",
    "Streaming with agents is a little more complex\n",
    "To construct the agent executor we need\n",
    "- Tools\n",
    "- ChatPromptTemplate\n",
    "- Our LLM\n",
    "- An Agent\n",
    "- Finally, the agent executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "142a433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOLS\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x\n",
    "\n",
    "@tool\n",
    "def final_answer(answer: str, tools_used: list[str]) -> str:\n",
    "    \"\"\"Use this tool to provide a final answer to the user.\n",
    "    The answer should be in natural language as this will be provided\n",
    "    to the user directly. The tools_used must include a list of tool\n",
    "    names that were used within the `scratchpad`. You MUST use this tool\n",
    "    to conclude the interaction.\n",
    "    \"\"\"\n",
    "    return {\"answer\": answer, \"tools_used\": tools_used}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "637f3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, subtract, multiply, exponentiate, final_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22d3fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You're a helpful assistant. When answering a user's question \"\n",
    "        \"you should first use one of the tools provided. After using a \"\n",
    "        \"tool the tool output will be provided back to you. You MUST \"\n",
    "        \"then use the final_answer tool to provide a final answer to the user. \"\n",
    "        \"DO NOT use the same tool more than once.\"\n",
    "    )),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT\n",
    "\n",
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools=tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "181dbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT EXECUTOR\n",
    "\n",
    "import json\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: str) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            out = self.agent.invoke({\n",
    "                \"input\": input,\n",
    "                \"chat_history\": self.chat_history,\n",
    "                \"agent_scratchpad\": agent_scratchpad\n",
    "            })\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if out.tool_calls[0][\"name\"] == \"final_answer\":\n",
    "                break\n",
    "            agent_scratchpad.append(out)  # add tool call to scratchpad\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_out = name2tool[out.tool_calls[0][\"name\"]](**out.tool_calls[0][\"args\"])\n",
    "            # add the tool output to the agent scratchpad\n",
    "            action_str = f\"The {out.tool_calls[0]['name']} tool returned {tool_out}\"\n",
    "            agent_scratchpad.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": action_str,\n",
    "                \"tool_call_id\": out.tool_calls[0][\"id\"]\n",
    "            })\n",
    "            # add a print so we can see intermediate steps\n",
    "            print(f\"{count}: {action_str}\")\n",
    "            count += 1\n",
    "        # add the final output to the chat history\n",
    "        final_answer = out.tool_calls[0][\"args\"]\n",
    "        # this is a dictionary, so we convert it to a string for compatibility with\n",
    "        # the chat history\n",
    "        final_answer_str = json.dumps(final_answer)\n",
    "        self.chat_history.append({\"input\": input, \"output\": final_answer_str})\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=final_answer_str)\n",
    "        ])\n",
    "        # return the final answer in dict form\n",
    "        return final_answer\n",
    "    \n",
    "agent_executor = CustomAgentExecutor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b178b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The add tool returned 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': '10 + 10 equals 20.', 'tools_used': ['functions.add']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\"What is 10+10?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLM needs a callback to handle how to give the generated tokens to the next code block\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    streaming=True\n",
    ").configurable_fields(\n",
    "    callbacks=ConfigurableField(\n",
    "        id=\"callbacks\",\n",
    "        name=\"callbacks\",\n",
    "        description=\"A list of callbacks to use for streaming\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f0d36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools=tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28923d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "class QueueCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\"\n",
    "    Callback handler that puts tokens into a queue\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, queue: asyncio.Queue):\n",
    "        self.queue = queue\n",
    "        self.final_answer_seen = False\n",
    "\n",
    "    async def __aiter__(self):\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                await asyncio.sleep(0.1)\n",
    "                continue\n",
    "            token_or_done = await self.queue.get()\n",
    "\n",
    "            if token_or_done == \"<<DONE>>\":\n",
    "                # this means we're done\n",
    "                return\n",
    "            if token_or_done:\n",
    "                yield token_or_done\n",
    "\n",
    "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Put new token in the queue.\n",
    "        \"\"\"\n",
    "        chunk = kwargs.get(\"chunk\")\n",
    "        if chunk:\n",
    "            # check for final answer tool call\n",
    "            if tool_calls := chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
    "                if tool_calls[0][\"function\"][\"name\"] == \"final_answer\":\n",
    "                    # this will allow the stream to end on the next 'on_llm_end' call\n",
    "                    self.final_answer_seen = True\n",
    "        self.queue.put_nowait(kwargs.get(\"chunk\"))\n",
    "        return\n",
    "    \n",
    "    async def on_llm_end(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Put None in the queue to signal completion\n",
    "        \"\"\"\n",
    "        # this should only be used at the end of our agent execution, however LangChain\n",
    "        # will call this at the end of every tool call, not just the final tool call\n",
    "        # so we must only send the \"done\" signal if we have already seen the final_answer\n",
    "        # tool call\n",
    "        if self.final_answer_seen:\n",
    "            self.queue.put_nowait(\"<<DONE>>\")\n",
    "        else:\n",
    "            self.queue.put_nowait(\"<<STEP_END>>\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b64a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_mfXCssI9X44wka8Lj6FdTRYI', 'function': {'arguments': '', 'name': 'add'}, 'type': 'function'}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' tool_calls=[{'name': 'add', 'args': {}, 'id': 'call_mfXCssI9X44wka8Lj6FdTRYI', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'add', 'args': '', 'id': 'call_mfXCssI9X44wka8Lj6FdTRYI', 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '{\"', 'name': None}, 'type': None}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}] tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'x', 'name': None}, 'type': None}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' invalid_tool_calls=[{'name': None, 'args': 'x', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': 'x', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' invalid_tool_calls=[{'name': None, 'args': '10', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '10', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ',\"', 'name': None}, 'type': None}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' invalid_tool_calls=[{'name': None, 'args': ',\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': ',\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'y', 'name': None}, 'type': None}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' invalid_tool_calls=[{'name': None, 'args': 'y', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': 'y', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '10', 'name': None}, 'type': None}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' invalid_tool_calls=[{'name': None, 'args': '10', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '10', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '}', 'name': None}, 'type': None}]} response_metadata={} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45' invalid_tool_calls=[{'name': None, 'args': '}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'service_tier': 'default'} id='run--e2b83914-fb44-4239-a23c-4b5678f21a45'\n"
     ]
    }
   ],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "async def stream(query: str):\n",
    "    response = agent.with_config(\n",
    "        callbacks=[streamer]\n",
    "    )\n",
    "    async for token in response.astream({\n",
    "        \"input\": query,\n",
    "        \"chat_history\": [],\n",
    "        \"agent_scratchpad\": []\n",
    "    }):\n",
    "        tokens.append(token)\n",
    "        print(token, flush=True)\n",
    "\n",
    "await stream(\"What is 10+10?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
