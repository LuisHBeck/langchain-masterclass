{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3302fc74",
   "metadata": {},
   "source": [
    "## LangChain - Query translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3cfcd7",
   "metadata": {},
   "source": [
    "### Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2670f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1864880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INDEXING ##\n",
    "\n",
    "# Load blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "# Make splits \n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# embedding\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# Index\n",
    "vector_store = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c09403",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354d04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt ##\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"\n",
    "    You are an AI language model assistant. Your task is to generate five different\n",
    "    versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is\n",
    "    to help the user overcome some of the limitations of the distance-based similarity\n",
    "    search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\n",
    "\"\"\"\n",
    "prompt_perspective = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspective\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcac09e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BEU7CA\\AppData\\Local\\Temp\\ipykernel_12248\\1830023186.py:7: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    }
   ],
   "source": [
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\"Unique union of retrieved docs\"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flatten_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique document\n",
    "    unique_docs = list(set(flatten_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260efe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c383d177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition for LLM (large language model) agents involves breaking down complex tasks into smaller, more manageable subgoals to enhance efficiency in handling intricate problems. This process can be facilitated by techniques such as Chain of Thought (CoT) prompting, which encourages the model to “think step by step.” CoT helps in transforming a large task into a series of simpler steps, thereby illuminating the model's thinking process.\\n\\nAdditionally, the Tree of Thoughts approach extends CoT by allowing the exploration of multiple reasoning possibilities at each step. It involves decomposing the problem into various thought steps and generating multiple thoughts per step, forming a tree structure that can be traversed using search methods like breadth-first search (BFS) or depth-first search (DFS).\\n\\nTask decomposition can be achieved through various means, including:\\n1. Simple prompting, such as asking for the steps needed to complete a task or identifying subgoals.\\n2. Task-specific instructions tailored to particular activities (e.g., creating an outline for a story).\\n3. Input from human users.\\n\\nOverall, task decomposition is crucial for LLM agents as it aids in organizing and simplifying complicated tasks, enabling better planning and execution.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"\"\n",
    "    Answer the following question based on this context:\n",
    "\n",
    "    {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc1c11a",
   "metadata": {},
   "source": [
    "### RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d7d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8cdefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Fusion: Related\n",
    "fusion_template = \"\"\"\n",
    "    You are a helpful assistant that generates multiple search queries based \n",
    "    on a single input query. \\n Generates a multiple search queries related to: \\n\n",
    "    {question}\n",
    "    Output (4 queries):\n",
    "\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(fusion_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a896b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries_fusion = (\n",
    "    prompt_rag_fusion\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73f48bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\n",
    "    Reciprocal_rank_fusion that takes multiple list of ranked\n",
    "    documents and an optional parameter k used in the RRF formula\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final ranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries_fusion | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d9ab8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (large language model) agents involves breaking down complex tasks into smaller, manageable subgoals. This process enables the agent to handle intricate tasks more efficiently. There are several methods for task decomposition:\\n\\n1. **Chain of Thought (CoT)**: This technique encourages the model to \"think step by step,\" allowing it to decompose difficult tasks into simpler steps, thereby enhancing its performance.\\n\\n2. **Tree of Thoughts**: This approach extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thoughts. It decomposes the problem into multiple thought steps and generates various thoughts per step, which can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS).\\n\\n3. **Prompting Techniques**: Task decomposition can be achieved through simple prompts, such as asking for the steps to achieve a goal or identifying subgoals.\\n\\n4. **Task-Specific Instructions**: Providing specific instructions tailored to the task, such as asking for a story outline when writing a novel.\\n\\n5. **Human Inputs**: Involving human guidance to assist in breaking down tasks.\\n\\nOverall, task decomposition is a crucial component that allows LLM agents to manage and execute complex tasks more effectively.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\"\n",
    "    Answer the following question based on this context:\n",
    "\n",
    "    {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_rag_fusion,\n",
    "        \"question\": itemgetter(\"question\") \n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb8100",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9876de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_decomposition = \"\"\"\n",
    "    You are a helpful assistant that generates multiple sub-questions related\n",
    "    to an input question. \\n The goal is to break down the input a set of sub-problems /\n",
    "    sub-questions that can be answered in isolation. \\n Generate multiple search queries\n",
    "    related to: {question}.\n",
    "    Output (3 queries): \n",
    "\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b58299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_query_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ddd011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the main components of an LLM-powered agent system?\"\n",
    "questions = generate_query_decomposition.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5195df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the key components of a large language model (LLM) architecture?',\n",
       " '2. How do LLMs integrate with other systems in an agent-based architecture?',\n",
       " '3. What role does data preprocessing play in the performance of an LLM-powered agent system?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17b68353",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\"\n",
    "    Here is the question you need to answer:\n",
    "    \\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "    Here is any available background question + answer pairs:\n",
    "    \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "    Here is the additional context relevant to the question:\n",
    "    \\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "    Use the above context and any background question + answer pairs to answer\n",
    "    the question: \\n {question}.\n",
    "\"\"\"\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f221abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    formatted_str = \"\"\n",
    "    formatted_str += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_str.strip()\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    rag_chain_decomposition = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever,\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"q_a_pairs\": itemgetter(\"q_a_pairs\")\n",
    "        }\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain_decomposition.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n --- \\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "391dd5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Data preprocessing plays a crucial role in the performance of an LLM-powered agent system by ensuring that the input data is clean, relevant, and structured in a way that maximizes the model's ability to understand and generate appropriate responses. Here are several key aspects of how data preprocessing impacts the performance of such systems:\\n\\n1. **Quality of Input Data**: Preprocessing helps filter out noise and irrelevant information from the input data. High-quality, well-structured data allows the LLM to learn more effectively from the training phase and perform better during inference. This is particularly important in agent systems where the LLM needs to interact with various external components and APIs.\\n\\n2. **Normalization and Standardization**: Preprocessing often involves normalizing and standardizing data formats, which helps the LLM interpret inputs consistently. This is essential for maintaining reliability in the natural language interface, as inconsistencies in data can lead to formatting errors or misinterpretations by the model.\\n\\n3. **Context Management**: Given the finite context length of LLMs, preprocessing can help manage the amount of historical information included in the input. By selecting the most relevant context and summarizing or condensing information, preprocessing ensures that the LLM can focus on the most pertinent details, enhancing its ability to generate coherent and contextually appropriate responses.\\n\\n4. **Task Decomposition**: Effective preprocessing can assist in breaking down complex tasks into smaller, manageable components. This aligns with the LLM's capability to decompose tasks into subgoals, facilitating more efficient problem-solving and planning within the agent system.\\n\\n5. **Error Reduction**: By preprocessing data to eliminate inconsistencies and errors, the likelihood of the LLM generating incorrect or nonsensical outputs is reduced. This is particularly important in agent systems where reliability is critical, as errors can lead to significant issues in task execution.\\n\\n6. **Training Efficiency**: Preprocessed data can improve the efficiency of the training process by providing the LLM with clear and relevant examples. This can lead to faster convergence during training and better generalization to unseen data, ultimately enhancing the agent's performance in real-world applications.\\n\\n7. **Facilitating Reflection and Refinement**: Preprocessing can also support the self-reflection and refinement processes of the LLM. By providing structured feedback and clear examples of past actions, the model can learn from its mistakes more effectively, leading to improved future performance.\\n\\nIn summary, data preprocessing is a foundational step that significantly influences the effectiveness and reliability of LLM-powered agent systems. By ensuring high-quality, relevant, and well-structured input data, preprocessing enhances the model's ability to understand context, generate accurate responses, and perform complex tasks efficiently.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6722c7b",
   "metadata": {},
   "source": [
    "### Step-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ccb3e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75ce9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Show Examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what cna the member of The Police do?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel's was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel's personal history?\"\n",
    "    },\n",
    "]\n",
    "\n",
    "examples_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=examples_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert world knowledge. Your task is to step back and paraphrase a question to more generic step-back question, which is easier to answer. Here a few examples: \"\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7017c8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What does task decomposition mean in the context of agents?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c01ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "635bceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (large language model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This approach enhances the agent\\'s ability to handle intricate problems by allowing it to focus on simpler components, thereby improving efficiency and effectiveness in task execution.\\n\\nThere are several techniques and methodologies for task decomposition in LLM agents:\\n\\n1. **Chain of Thought (CoT)**: This standard prompting technique encourages the model to \"think step by step.\" By doing so, it utilizes more computational resources at test time to decompose difficult tasks into simpler, sequential steps. This method not only aids in task management but also provides insights into the model\\'s reasoning process.\\n\\n2. **Tree of Thoughts**: An extension of CoT, this approach explores multiple reasoning possibilities at each step. It decomposes the problem into various thought steps and generates multiple thoughts for each step, creating a tree structure. The search process can be conducted using breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.\\n\\n3. **Prompting Techniques**: Task decomposition can be achieved through simple prompting, such as asking the LLM to list the steps for a specific task or to identify subgoals. Additionally, task-specific instructions can guide the model, or human inputs can be utilized to define the decomposition process.\\n\\n4. **LLM+P Approach**: This method involves using an external classical planner for long-horizon planning. The LLM translates the problem into a Planning Domain Definition Language (PDDL) format, requests a classical planner to generate a plan, and then translates that plan back into natural language. This approach is particularly useful in domains where domain-specific PDDL and suitable planners are available.\\n\\nOverall, task decomposition is a critical component of LLM-powered autonomous agents, enabling them to tackle complex tasks more effectively by breaking them down into simpler, actionable parts.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_prompt_template = \"\"\"\n",
    "    You are an expert of world knowledge. I am going to ask you a question.\n",
    "    Your response should be comprehensive and not contradicted with the following \n",
    "    context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "    # {normal_context}\n",
    "    # {step_back_context}\n",
    "\n",
    "    # Original question: {question}\n",
    "    # Answer: \n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain_step_back = (\n",
    "    {\n",
    "        # retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65e747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
