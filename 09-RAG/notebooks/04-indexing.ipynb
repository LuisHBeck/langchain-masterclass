{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49bc4121",
   "metadata": {},
   "source": [
    "## LangChain - Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad46963",
   "metadata": {},
   "source": [
    "### Multi-representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606cd5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a35c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | AzureChatOpenAI(model=\"gpt-4o-mini\",api_version=\"2024-12-01-preview\", max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68983b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The document titled \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the design and capabilities of autonomous agents powered by large language models (LLMs). It outlines the key components necessary for these agents, including:\\n\\n1. **Planning**: \\n   - Task decomposition, breaking complex tasks into manageable subgoals, and using techniques like Chain of Thoughts (CoT) for better reasoning.\\n   - Self-reflection to iteratively improve decision-making based on previous actions.\\n\\n2. **Memory**:\\n   - Incorporating types of memory analogous to human cognition.\\n   - Use of short-term memory (in-context learning) and long-term memory via external vector storage, with Maximum Inner Product Search (MIPS) techniques to aid in efficient information retrieval.\\n\\n3. **Tool Use**:\\n   - Enabling LLMs to utilize external tools for tasks beyond their pretrained capabilities, enhancing their functionality and enabling complex operations.\\n   - Examples of projects like MRKL and HuggingGPT that demonstrate task management and tool integration.\\n\\nWeng also addresses various challenges in developing these agents, such as:\\n- Limited context capacity restricting the implementation of historical learning and reflection.\\n- Difficulties in long-term planning and adapting to unexpected errors.\\n- Reliability issues associated with the natural language interfaces used for communication among the components.\\n\\nSeveral proof-of-concept examples illustrate the effectiveness of LLMs in autonomous systems, including the scientific discovery agent ChemCrow and generative agents simulating human behavior.\\n\\nThe document includes references and citations to numerous research papers, highlighting the ongoing advancements in this field. Overall, it emphasizes the potential of harnessing LLMs for creating intelligent autonomous agents capable of performing complex tasks efficiently.',\n",
       " 'The document, \"Thinking about High-Quality Human Data,\" authored by Lilian Weng, emphasizes the critical role of high-quality human data in training machine learning models, particularly deep learning approaches. It outlines two principal areas for improving data quality: the intricate process of human raters and the interplay between data quality and model training.\\n\\n### Key Themes:\\n1. **Human Raters and Data Quality**: \\n   - The process of collecting human-labeled data is multifaceted, involving task design, rater selection and training, and data aggregation. \\n   - It underscores the importance of quality assurance through clear guidelines, consistent rater performance assessments, and the aggregation techniques that help identify true labels.\\n\\n2. **Wisdom of the Crowd**: \\n   - The document references historical studies that illustrate how collective judgments can often yield reliable outcomes despite individual inaccuracies or biases.\\n\\n3. **Rater Agreement and Disagreement**: \\n   - Different methods for evaluating and utilizing rater agreement are discussed, including majority voting, Cohen\\'s Kappa, and probabilistic models for estimating rater competence.\\n   - It acknowledges that rater disagreement can be legitimate and informative, leading to the proposal of two paradigms: descriptive (embracing subjectivity) and prescriptive (seeking consistency).\\n\\n4. **Data Quality and Model Training**: \\n   - The document presents various methods to evaluate and manage data quality through model training dynamics, such as influence functions, prediction changes, and noisy cross-validation.\\n   - Techniques are introduced to identify mislabeled data, focusing on attributes like model confidence and prediction margins.\\n\\n5. **Complexity in Data Annotation**:\\n   - It highlights contrasting views on data annotation, noting that while “ground truth” may be static in some contexts, it can evolve, particularly in complex and subjective tasks.\\n\\n6. **Innovative Approaches**: \\n   - Methods like Jury Learning, which take into account annotator demographics and inconsistencies, and the generative models used to predict raters\\' behaviors are discussed, suggesting avenues for improving data collection processes.\\n\\nOverall, the document presents a thorough analysis of the intricacies involved in gathering high-quality human data for machine learning, framing it as a critical yet often overlooked element of ML success. It emphasizes that effective data work is vital and worthy of equal focus as model development within the AI community.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fde3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BEU7CA\\AppData\\Local\\Temp\\ipykernel_29444\\2897780424.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vector store to use to index the child chinks\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"summaries\",\n",
    "    embedding_function=AzureOpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    byte_store=store,\n",
    "    id_key=id_key # key between the chunks and full docs\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088318e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '782b5b8d-3dc1-41c1-8ee1-bbec92dbc70b'}, page_content='The document titled \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the design and capabilities of autonomous agents powered by large language models (LLMs). It outlines the key components necessary for these agents, including:\\n\\n1. **Planning**: \\n   - Task decomposition, breaking complex tasks into manageable subgoals, and using techniques like Chain of Thoughts (CoT) for better reasoning.\\n   - Self-reflection to iteratively improve decision-making based on previous actions.\\n\\n2. **Memory**:\\n   - Incorporating types of memory analogous to human cognition.\\n   - Use of short-term memory (in-context learning) and long-term memory via external vector storage, with Maximum Inner Product Search (MIPS) techniques to aid in efficient information retrieval.\\n\\n3. **Tool Use**:\\n   - Enabling LLMs to utilize external tools for tasks beyond their pretrained capabilities, enhancing their functionality and enabling complex operations.\\n   - Examples of projects like MRKL and HuggingGPT that demonstrate task management and tool integration.\\n\\nWeng also addresses various challenges in developing these agents, such as:\\n- Limited context capacity restricting the implementation of historical learning and reflection.\\n- Difficulties in long-term planning and adapting to unexpected errors.\\n- Reliability issues associated with the natural language interfaces used for communication among the components.\\n\\nSeveral proof-of-concept examples illustrate the effectiveness of LLMs in autonomous systems, including the scientific discovery agent ChemCrow and generative agents simulating human behavior.\\n\\nThe document includes references and citations to numerous research papers, highlighting the ongoing advancements in this field. Overall, it emphasizes the potential of harnessing LLMs for creating intelligent autonomous agents capable of performing complex tasks efficiently.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "\n",
    "sub_docs = vector_store.similarity_search(query=query, k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4156fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BEU7CA\\AppData\\Local\\Temp\\ipykernel_29444\\17140411.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query, n_results=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three:\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query, n_results=1)\n",
    "retrieved_docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb028c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
